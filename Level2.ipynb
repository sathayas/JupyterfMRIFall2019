{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align:center'>\n",
    "PSY 394U <b>Methods for fMRI</b>, Fall 2019\n",
    "\n",
    "\n",
    "<img style='width: 300px; padding: 0px;' src='https://github.com/sathayas/JupyterfMRIFall2019/blob/master/Images/Placebo_Left.png?raw=true' alt='brain blobs'/>\n",
    "\n",
    "</p>\n",
    "\n",
    "<p style='text-align:center; font-size:40px; margin-bottom: 30px;'><b> Second and higher level analyses </b></p>\n",
    "\n",
    "<p style='text-align:center; font-size:18px; margin-bottom: 32px;'><b> October 21 - 28, 2019</b></p>\n",
    "\n",
    "<hr style='height:5px;border:none' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second-level analysis, finger-foot-lips data\n",
    "<hr style=\"height:1px;border:none\" />\n",
    "\n",
    "## Data\n",
    "For this example, we will use the first-level analysis results from the finger-foot-lips task from the test-retest data set (ds114). The first-level analysis has been run for all the subjects, and their respective `.feat` directories have been generated. You can find them in the tar ball `ds114_BatchOutput_FingerFootLips.tar.gz`, available for download from [Box.com](https://utexas.box.com/s/3gjwyymbtkndnx85q0l3p83chgh0rwcz). \n",
    "\n",
    "Once you download the file, move it to the `ds114` directory on your local computer. Then you can unpack this tar ball (either locally on your computer, or via Docker) by running:\n",
    "```\n",
    "gunzip ds114_BatchOutput_FingerFootLips.tar.gz\n",
    "```\n",
    "Then\n",
    "```\n",
    "tar -xvf ds114_BatchOutput_FingerFootLips.tar\n",
    "```\n",
    "This should create a directory called `BatchOutput_FingerFootLips` under the `ds114` directory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "As you recall from the previous class, there 7 different T contrasts estimated during the first-level analysis. We will focus on contrast 5 (finger > others, referred as cope5 hereafter). The goal of the second-level analysis is to see if all the subjects had the same activation patter. If the same brain area is activated across subjects, then we will be able to see it in the second-level analysis results.\n",
    "\n",
    "First, we import necessary libraries, and define directories and parameters to be used in this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`<Level2_FingerFootLips_Test_Cope5.py>`](https://github.com/sathayas/fMRIClassFall2019/blob/master/Level2/Level2_FingerFootLips_Test_Cope5.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # system functions\n",
    "import nipype.interfaces.fsl as fsl  # fsl\n",
    "from nipype import Node, Workflow  # components to construct workflow\n",
    "from nipype import SelectFiles  # to facilitate file i/o\n",
    "from nipype.interfaces.io import DataSink  # datasink\n",
    "\n",
    "\n",
    "##### PARAMETERS #####\n",
    "indCope = '5'  # the contrast of interest, finger vs others\n",
    "indSes = 'test'  # the session of interest\n",
    "\n",
    "\n",
    "##### DIRECTORY BUSINESS ######\n",
    "# original data directory\n",
    "dataDir = '/tmp/Data/ds114'\n",
    "# Output directory\n",
    "outDir = os.path.join(dataDir,'WorkflowOutput')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In preparation for the second-level analysis, we have to combine contrast images (cope images) and variance image (varcope images) across subjects. We do so by concatenating the `cope5.nii.gz` and `varcope5.nii.gz` images from all the subjects as 4D images, with subjects corresponding to the t-direction (or time points). In other words, \n",
    "   \n",
    "   * time point 1: Subject 01\n",
    "   * time point 2: Subject 02\n",
    "   * ...\n",
    "   * time point 10: Subject 10\n",
    "\n",
    "This is done by the **`Merge`** tool from FSL. In addition to the `cope` and `varcope` images, we also combine mask images across subjects. Unfortunately the first-level analysis results are not stored in a BIDS-compliant file structure, you have to hard code to specify the appropriate directories and files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "#\n",
    "# A LIST OF COPE, VARCOPE, AND MASK FILES TO BE MEREGED\n",
    "#\n",
    "###########\n",
    "# directory where preprocessed fMRI data is located\n",
    "baseDir = os.path.join(dataDir, 'BatchOutput_FingerFootLips/feat_dir')\n",
    "\n",
    "# a list of subjects\n",
    "subject_list = ['%02d' % i for i in range(1,11)]\n",
    "\n",
    "listCopeFiles = []\n",
    "listVarcopeFiles = []\n",
    "listMaskFiles = []\n",
    "for iSubj in subject_list:\n",
    "    # full path to a cope image\n",
    "    pathCope = os.path.join(baseDir,\n",
    "                            'sub-' + iSubj,\n",
    "                            'ses-' + indSes,\n",
    "                            'run0.feat',\n",
    "                            'stats',\n",
    "                            'cope' + indCope + '.nii.gz')\n",
    "    listCopeFiles.append(pathCope)\n",
    "\n",
    "    # full path to a varcope image\n",
    "    pathVarcope = os.path.join(baseDir,\n",
    "                            'sub-' + iSubj,\n",
    "                            'ses-' + indSes,\n",
    "                            'run0.feat',\n",
    "                            'stats',\n",
    "                            'varcope' + indCope + '.nii.gz')\n",
    "    listVarcopeFiles.append(pathVarcope)\n",
    "\n",
    "    # full path to a mask image\n",
    "    pathMask = os.path.join(baseDir,\n",
    "                            'sub-' + iSubj,\n",
    "                            'ses-' + indSes,\n",
    "                            'run0.feat',\n",
    "                            'mask.nii.gz')\n",
    "    listMaskFiles.append(pathMask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "#\n",
    "# NODES FOR THE MERGING IMAGES\n",
    "#\n",
    "###########\n",
    "# merging cope files\n",
    "copemerge = Node(fsl.Merge(dimension='t',\n",
    "                           in_files=listCopeFiles),\n",
    "                 name=\"copemerge\")\n",
    "\n",
    "# merging varcope files\n",
    "varcopemerge = Node(fsl.Merge(dimension='t',\n",
    "                           in_files=listVarcopeFiles),\n",
    "                    name=\"varcopemerge\")\n",
    "\n",
    "# merging mask files\n",
    "maskmerge = Node(fsl.Merge(dimension='t',\n",
    "                           in_files=listMaskFiles),\n",
    "                 name=\"maskmerge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are merging mask images so that we can include only the brain voxels in our second-level analysis. As you can imagine, different subjects have different voxels included in their brain mask.\n",
    "\n",
    "<p style='text-align:center; font-size:18px;'>(Scroll through merged mask images)</p>\n",
    "\n",
    "For the second-level analysis, we want to include voxels that are present in all subjects. So we need to create another mask image specifically for the second-level analysis, indicating only the voxels that are present in all subjects. This is done by generating the minimum image (across time-axis) of the merged mask image.\n",
    "\n",
    "### Exercise\n",
    "1. **Why minimum image?** Can you explain why the minimum image from the merged mask image contain only the voxels that are present in all subjects?  \n",
    "\n",
    "<p style='text-align:center; font-size:18px;'>(Show the minimum mask image)</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the minimum across time points on merged mask image\n",
    "minmask = Node(fsl.MinImage(),\n",
    "               name=\"minmask\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to specify the regression model. In this analysis, we are simply doing a one-sample t-test, so the regression model should only include the intercept (or the mean). The regression model is specified by a dictionary of regressors. Each element in the dictionary is a regressor. They key corresponds to the name of the regressor, and the values contains a list of numbers, in the same order as the merged cope and varcope images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "#\n",
    "# SETTING UP THE SECOND LEVEL ANALYSIS NODES\n",
    "#\n",
    "###########\n",
    "# Dictionary with regressors\n",
    "dictReg = {'reg1': [1]*len(subject_list) # vector of ones\n",
    "          }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the contrast, since we only have one regressor, we can only do activation (i.e., \\[1\\]) and deactivation (i.e., \\[-1\\])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contrasts\n",
    "cont01 = ['activation', 'T', list(dictReg.keys()), [1]]\n",
    "cont02 = ['activation', 'T', list(dictReg.keys()), [-1]]\n",
    "\n",
    "contrastList = [cont01, cont02]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the regression model (i.e., `dictReg`) and the contrasts (i.e., `contrastList`), we can set up the second level analysis. This is done by the **`MultipleRegressionModel`** tool in FSL. This produces necessary files for calculating various contrast and statistic images for the second-level analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the second level analysis model node\n",
    "level2design = Node(fsl.MultipleRegressDesign(contrasts=contrastList,\n",
    "                                              regressors=dictReg),\n",
    "                    name='level2design')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual calculation of statistic images is done by the **`FLAMEO`** tool in FSL. This step only requires a fixed-effect analysis (thus `run_mode='fe'`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model calculation by FLAMEO\n",
    "flameo = Node(fsl.FLAMEO(run_mode='fe'),\n",
    "              name=\"flameo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a datasink, and a workflow object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating datasink to collect outputs\n",
    "datasink = Node(DataSink(base_directory=\n",
    "                         os.path.join(outDir,'FingerFootLips_Test_Cope5')),\n",
    "                name='datasink')\n",
    "\n",
    "\n",
    "###########\n",
    "#\n",
    "# SETTING UP THE WORKFLOW NODES\n",
    "#\n",
    "###########\n",
    "\n",
    "# creating the workflow\n",
    "secondLevel = Workflow(name=\"Level2\", base_dir=outDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now actually connecting nodes. First, we pass on various files describing the second-level analysis mode from `level2design` node to `flameo` node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting nodes\n",
    "secondLevel.connect(level2design, 'design_mat', flameo, 'design_file')\n",
    "secondLevel.connect(level2design, 'design_con', flameo, 't_con_file')\n",
    "secondLevel.connect(level2design, 'design_grp', flameo, 'cov_split_file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, merged cope and varcope images are passed onto the `flameo` node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondLevel.connect(copemerge, 'merged_file', flameo, 'cope_file')\n",
    "secondLevel.connect(varcopemerge, 'merged_file', flameo, 'var_cope_file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the minimum of the merged mask image, and pass onto `flameo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondLevel.connect(maskmerge, 'merged_file', minmask, 'in_file')\n",
    "secondLevel.connect(minmask, 'out_file', flameo, 'mask_file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And sending a directory of statistic images (`stats_dir`) to the datasink."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondLevel.connect(flameo, 'stats_dir', datasink, 'stats_dir')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we run the workflow. It should run fairly quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the workflow\n",
    "secondLevel.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
